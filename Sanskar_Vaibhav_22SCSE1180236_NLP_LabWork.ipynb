{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxuyods8FBjYVFYL8109AK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanskarVaibhav/.ipython/blob/main/Sanskar_Vaibhav_22SCSE1180236_NLP_LabWork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP LABWORK"
      ],
      "metadata": {
        "id": "LTVaLd3IDS07"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmhcB9O3wS3D",
        "outputId": "a18c4337-8361-4364-f9f9-813daa9ef3d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens ['Text', 'preprocessing', 'is', 'essintial', 'in', 'NLP', '!']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(\"Text preprocessing is essintial in NLP!\")\n",
        "tokens= [token.text for token in doc]\n",
        "print(\"Tokens\",tokens)\n",
        "# ['Text', 'preprocessing', 'is', 'essintial', 'in', 'NLP'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7-7F_7gxlIR",
        "outputId": "e7f5421f-701a-484a-c15f-787ecf7d6ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "nltk.data.path.append('/root/nltk_data')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joRMEZ7Oy7NO",
        "outputId": "f6cc8c1e-19ef-4f76-f33f-78f4e28b54b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Natural Language Processing is fun. Let's learn it!\"\n",
        "\n",
        "tokens = text.split() #Simple word tokenization\n",
        "\n",
        "print(\"Word Tokenization:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atZaWoA6zKLK",
        "outputId": "e74c37b0-1634-45ca-ee5b-6a95c44174f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenization: ['Natural', 'Language', 'Processing', 'is', 'fun.', \"Let's\", 'learn', 'it!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXP 1 : Text Preprocessing & Tokenization"
      ],
      "metadata": {
        "id": "qjpmEvT0Dw7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Download the punkt_tab resource\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import string\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    return text\n",
        "# Sample text\n",
        "text = \"Hello! This is an example sentence, showing text preprocessing & tokenization.\"\n",
        "# Preprocess the text\n",
        "cleaned_text = preprocess_text(text)\n",
        "# Tokenize into sentences\n",
        "sentence_tokens = sent_tokenize(text)\n",
        "# Tokenize into words\n",
        "word_tokens = word_tokenize(cleaned_text)\n",
        "print(\"Sentence Tokens:\")\n",
        "print(sentence_tokens)\n",
        "print(\"\\nWord Tokens:\")\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "WBMl0G7g6yQo",
        "outputId": "8112fc42-a449-4782-a6ed-04e3e8cd0e08"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokens:\n",
            "['Hello!', 'This is an example sentence, showing text preprocessing & tokenization.']\n",
            "\n",
            "Word Tokens:\n",
            "['hello', 'this', 'is', 'an', 'example', 'sentence', 'showing', 'text', 'preprocessing', 'tokenization']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXP 2 : Parts of Speech (POS) Tagging"
      ],
      "metadata": {
        "id": "2jHkIDDREvva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "def tag_pos(tokens):\n",
        "    return pos_tag(tokens)\n",
        "text = \"The quick brown fox jumps.\"\n",
        "tokens = tokenize_text(text)\n",
        "pos_tags = tag_pos(tokens)\n",
        "print(pos_tags)\n",
        "sample_text = \"I love coding daily.\"\n",
        "sample_tokens = tokenize_text(sample_text)\n",
        "sample_tags = tag_pos(sample_tokens)\n",
        "print(sample_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "cofY23zvFTWS",
        "outputId": "561dc5b3-7611-4565-e4f2-5875d8df1eb9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'NNS'), ('.', '.')]\n",
            "[('I', 'PRP'), ('love', 'VBP'), ('coding', 'VBG'), ('daily', 'RB'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp 3 : Named Entity Recognition (NER) Implementation"
      ],
      "metadata": {
        "id": "5L4uTsipF1YE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk import ne_chunk\n",
        "\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "def tag_pos(tokens):\n",
        "    return pos_tag(tokens)\n",
        "\n",
        "def extract_entities(tags):\n",
        "    return ne_chunk(tags)\n",
        "\n",
        "text = \"Elon Musk works at Tesla.\"\n",
        "tokens = tokenize_text(text)\n",
        "pos_tags = tag_pos(tokens)\n",
        "ner_tree = extract_entities(pos_tags)\n",
        "print(ner_tree)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "q6Olf6t7Fz0A",
        "outputId": "4f6e74a7-0259-409d-b42b-30c7f26139dc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Elon/NNP)\n",
            "  (ORGANIZATION Musk/NNP)\n",
            "  works/VBZ\n",
            "  at/IN\n",
            "  (ORGANIZATION Tesla/NNP)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXP 4 : N-Gram Language Modeling"
      ],
      "metadata": {
        "id": "ZoQkAbAeGWKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "def generate_bigrams(tokens):\n",
        "    return list(ngrams(tokens, 2))\n",
        "\n",
        "def generate_trigrams(tokens):\n",
        "    return list(ngrams(tokens, 3))\n",
        "\n",
        "text = \"I love to learn NLP.\"\n",
        "tokens = tokenize_text(text)\n",
        "bigrams = generate_bigrams(tokens)\n",
        "print(bigrams)\n",
        "\n",
        "sample_text = \"This is a test case.\"\n",
        "sample_tokens = tokenize_text(sample_text)\n",
        "trigrams = generate_trigrams(sample_tokens)\n",
        "print(trigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zGgNNLTdHEeV",
        "outputId": "ca6ddaca-6e06-426f-fe0d-df523079e579"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'love'), ('love', 'to'), ('to', 'learn'), ('learn', 'NLP'), ('NLP', '.')]\n",
            "[('This', 'is', 'a'), ('is', 'a', 'test'), ('a', 'test', 'case'), ('test', 'case', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp 5: Word Representation - One-Hot Encoding vs. Bag of Words"
      ],
      "metadata": {
        "id": "lX72LaMUHDtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def create_vectorizer():\n",
        "    return CountVectorizer()\n",
        "\n",
        "def fit_transform_texts(vectorizer, texts):\n",
        "    return vectorizer.fit_transform(texts)\n",
        "\n",
        "def get_feature_names(vectorizer):\n",
        "    return vectorizer.get_feature_names_out()\n",
        "\n",
        "def convert_to_array(matrix):\n",
        "    return matrix.toarray()\n",
        "\n",
        "vectorizer = create_vectorizer()\n",
        "corpus = [\"I love NLP\", \"NLP is great\"]\n",
        "bow_matrix = fit_transform_texts(vectorizer, corpus)\n",
        "features = get_feature_names(vectorizer)\n",
        "\n",
        "print(features)\n",
        "print(convert_to_array(bow_matrix))\n",
        "\n",
        "sample_corpus = [\"This is fun\", \"Fun work\"]\n",
        "sample_matrix = fit_transform_texts(vectorizer, sample_corpus)\n",
        "sample_features = get_feature_names(vectorizer)\n",
        "\n",
        "print(sample_features)\n",
        "print(convert_to_array(sample_matrix))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LPPuYJoDGg4e",
        "outputId": "48bd9007-aa01-4640-f2c6-9fe42cb48780"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['great' 'is' 'love' 'nlp']\n",
            "[[0 0 1 1]\n",
            " [1 1 0 1]]\n",
            "['fun' 'is' 'this' 'work']\n",
            "[[1 1 1 0]\n",
            " [1 0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp 6 : Word Sense Disambiguation (WSD) using Lesk Algorithm"
      ],
      "metadata": {
        "id": "Ni_12YqlHO0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.wsd import lesk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "def disambiguate_word(tokens, word):\n",
        "    return lesk(tokens, word)\n",
        "\n",
        "def get_sense_definition(sense):\n",
        "    return sense.definition()\n",
        "\n",
        "text = \"I deposited money in the bank.\"\n",
        "tokens = tokenize_text(text)\n",
        "sense = disambiguate_word(tokens, \"bank\")\n",
        "print(sense)\n",
        "print(get_sense_definition(sense))\n",
        "\n",
        "sample_text = \"The fish swam near the bank.\"\n",
        "sample_tokens = tokenize_text(sample_text)\n",
        "sample_sense = disambiguate_word(sample_tokens, \"bank\")\n",
        "print(sample_sense)\n",
        "print(get_sense_definition(sample_sense))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BY0mxwXEF3Ue",
        "outputId": "d7ee8029-028a-44c1-853d-923b0a0ce346"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('savings_bank.n.02')\n",
            "a container (usually with a slot in the top) for keeping money at home\n",
            "Synset('savings_bank.n.02')\n",
            "a container (usually with a slot in the top) for keeping money at home\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp 7 : Sentiment Analysis using Lexicon-Based Approach"
      ],
      "metadata": {
        "id": "WS2lQGWoHe7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "def create_analyzer():\n",
        "    return SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_sentiment_scores(analyzer, text):\n",
        "    return analyzer.polarity_scores(text)\n",
        "\n",
        "analyzer = create_analyzer()\n",
        "text = \"I love this movie, it's great!\"\n",
        "scores = get_sentiment_scores(analyzer, text)\n",
        "print(scores)\n",
        "\n",
        "sample_text = \"This product is terrible.\"\n",
        "sample_scores = get_sentiment_scores(analyzer, sample_text)\n",
        "print(sample_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DHZF90sQFWKe",
        "outputId": "7c18de24-ae7f-4d40-d9f2-01aacc9651b7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.0, 'neu': 0.259, 'pos': 0.741, 'compound': 0.8622}\n",
            "{'neg': 0.508, 'neu': 0.492, 'pos': 0.0, 'compound': -0.4767}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp 8 : Text Summarization using TF-IDF"
      ],
      "metadata": {
        "id": "yYEjm1-nHyH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def create_tfidf_vectorizer():\n",
        "    return TfidfVectorizer()\n",
        "\n",
        "def fit_transform_texts(vectorizer, texts):\n",
        "    return vectorizer.fit_transform(texts)\n",
        "\n",
        "def get_feature_names(vectorizer):\n",
        "    return vectorizer.get_feature_names_out()\n",
        "\n",
        "def convert_to_array(matrix):\n",
        "    return matrix.toarray()\n",
        "\n",
        "vectorizer = create_tfidf_vectorizer()\n",
        "corpus = [\"NLP is great\", \"I love NLP\"]\n",
        "tfidf_matrix = fit_transform_texts(vectorizer, corpus)\n",
        "features = get_feature_names(vectorizer)\n",
        "\n",
        "print(features)\n",
        "print(convert_to_array(tfidf_matrix))\n",
        "\n",
        "sample_corpus = [\"This is fun\", \"Fun with NLP\"]\n",
        "sample_matrix = fit_transform_texts(vectorizer, sample_corpus)\n",
        "sample_features = get_feature_names(vectorizer)\n",
        "\n",
        "print(sample_features)\n",
        "print(convert_to_array(sample_matrix))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9ZVh73LqFDFl",
        "outputId": "55f24832-0ec3-4e79-8d25-856d5bd2fdf6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['great' 'is' 'love' 'nlp']\n",
            "[[0.6316672  0.6316672  0.         0.44943642]\n",
            " [0.         0.         0.81480247 0.57973867]]\n",
            "['fun' 'is' 'nlp' 'this' 'with']\n",
            "[[0.44943642 0.6316672  0.         0.6316672  0.        ]\n",
            " [0.44943642 0.         0.6316672  0.         0.6316672 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp 9 : Question Answering using Rule-Based Approach"
      ],
      "metadata": {
        "id": "gvJ5gRo3IDUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "    return text.lower()\n",
        "\n",
        "def check_question_type(text):\n",
        "    return \"who\" in text\n",
        "\n",
        "def answer_who_question():\n",
        "    return \"Elon Musk\"\n",
        "\n",
        "def answer_default():\n",
        "    return \"I don't know\"\n",
        "\n",
        "text = \"Who is the CEO of Tesla?\"\n",
        "normalized = normalize_text(text)\n",
        "if check_question_type(normalized):\n",
        "    print(answer_who_question())\n",
        "else:\n",
        "    print(answer_default())\n",
        "\n",
        "sample_text = \"What is NLP?\"\n",
        "sample_normalized = normalize_text(sample_text)\n",
        "if check_question_type(sample_normalized):\n",
        "    print(answer_who_question())\n",
        "else:\n",
        "    print(answer_default())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "75_p3nUHExAy",
        "outputId": "84baee62-f363-4e0f-9af6-59d0cea275ad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elon Musk\n",
            "I don't know\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp 10 : Machine Translation using Rule-Based Approach"
      ],
      "metadata": {
        "id": "o0JLEG9cIWdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dictionary():\n",
        "    return {\"hello\": \"bonjour\", \"world\": \"monde\", \"i\": \"je\", \"love\": \"aime\"}\n",
        "\n",
        "def split_text(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "def translate_word(word, dictionary):\n",
        "    return dictionary.get(word, word)\n",
        "\n",
        "def join_words(words):\n",
        "    return \" \".join(words)\n",
        "\n",
        "dictionary = create_dictionary()\n",
        "text = \"Hello world\"\n",
        "words = split_text(text)\n",
        "translated = []\n",
        "for word in words:\n",
        "    translated.append(translate_word(word, dictionary))\n",
        "result = join_words(translated)\n",
        "print(result)\n",
        "\n",
        "sample_text = \"I love coding\"\n",
        "sample_words = split_text(sample_text)\n",
        "sample_translated = [translate_word(w, dictionary) for w in sample_words]\n",
        "sample_result = join_words(sample_translated)\n",
        "print(sample_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9EidYjz7EgDU",
        "outputId": "6d0c6f7b-d47d-4152-9e3a-268eeacbf5f2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bonjour monde\n",
            "je aime coding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp 11 : Statistical Machine Translation (SMT)"
      ],
      "metadata": {
        "id": "-Og62hodIsfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prob_dict():\n",
        "    return {\"hello\": {\"hola\": 0.9, \"adios\": 0.1}, \"world\": {\"mundo\": 0.8}}\n",
        "\n",
        "def split_text(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "def get_best_translation(word, prob_dict):\n",
        "    translations = prob_dict.get(word, {word: 1.0})\n",
        "    return max(translations, key=translations.get)\n",
        "\n",
        "def translate_text(words, prob_dict):\n",
        "    return [get_best_translation(word, prob_dict) for word in words]\n",
        "\n",
        "def join_words(words):\n",
        "    return \" \".join(words)\n",
        "\n",
        "prob_dict = create_prob_dict()\n",
        "text = \"Hello world\"\n",
        "words = split_text(text)\n",
        "translated = translate_text(words, prob_dict)\n",
        "result = join_words(translated)\n",
        "print(result)\n",
        "\n",
        "sample_text = \"Hello everyone\"\n",
        "sample_words = split_text(sample_text)\n",
        "sample_translated = translate_text(sample_words, prob_dict)\n",
        "sample_result = join_words(sample_translated)\n",
        "print(sample_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "WOB98GHsENo3",
        "outputId": "9fa842e8-1c6a-47aa-af8e-67b6cd4dfa28"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hola mundo\n",
            "hola everyone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp 12 : Morphological Analysis using Rule-Based Approach"
      ],
      "metadata": {
        "id": "RAsHDmYiI4y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prefixes():\n",
        "    return [\"un\", \"re\", \"in\"]\n",
        "\n",
        "def get_suffixes():\n",
        "    return [\"ing\", \"ed\", \"ly\"]\n",
        "\n",
        "def normalize_word(word):\n",
        "    return word.lower()\n",
        "\n",
        "def find_prefix(word, prefixes):\n",
        "    for p in prefixes:\n",
        "        if word.startswith(p):\n",
        "            return p, word[len(p):]\n",
        "    return \"\", word\n",
        "\n",
        "def find_suffix(word, suffixes):\n",
        "    for s in suffixes:\n",
        "        if word.endswith(s):\n",
        "            return word[:-len(s)], s\n",
        "    return word, \"\"\n",
        "\n",
        "word = \"unhappily\"\n",
        "prefixes = get_prefixes()\n",
        "suffixes = get_suffixes()\n",
        "word = normalize_word(word)\n",
        "prefix, root = find_prefix(word, prefixes)\n",
        "root, suffix = find_suffix(root, suffixes)\n",
        "print((prefix, root, suffix))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MsDAQ1YTDx3-",
        "outputId": "d9eafc41-6370-44e4-a42e-17f4f149ac2c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('un', 'happi', 'ly')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp 13 : Relation Extraction in Text  "
      ],
      "metadata": {
        "id": "V_v2AITgJDVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "def find_works_index(words):\n",
        "    return words.index(\"works\") if \"works\" in words else -1\n",
        "\n",
        "def find_at_index(words):\n",
        "    return words.index(\"at\") if \"at\" in words else -1\n",
        "\n",
        "def extract_person(words, works_idx):\n",
        "    return \" \".join(words[:works_idx])\n",
        "\n",
        "def extract_organization(words, at_idx):\n",
        "    return \" \".join(words[at_idx + 1:])\n",
        "\n",
        "def extract_relation(words):\n",
        "    works_idx = find_works_index(words)\n",
        "    at_idx = find_at_index(words)\n",
        "    if works_idx != -1 and at_idx != -1:\n",
        "        return extract_person(words, works_idx), \"works at\", extract_organization(words, at_idx)\n",
        "    return None\n",
        "\n",
        "text = \"Elon Musk works at Tesla\"\n",
        "words = split_text(text)\n",
        "result = extract_relation(words)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5ANAvTxxDhU9",
        "outputId": "4fc9cd55-e82d-4a37-fdfb-ab9f5564b09d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('elon musk', 'works at', 'tesla')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp 14 : Ethics and Bias in NLP - Gender Bias Analysis"
      ],
      "metadata": {
        "id": "dJe4ZcuvJRWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text(text):\n",
        "    return text.lower().split()\n",
        "def find_engineer_index(words):\n",
        "    return words.index(\"engineer\") if \"engineer\" in words else -1\n",
        "def get_next_word(words, idx):\n",
        "    return words[idx + 1] if idx + 1 < len(words) else \"\"\n",
        "def check_pronoun(word):\n",
        "    return word if word in [\"he\", \"she\"] else \"neutral\"\n",
        "def analyze_bias(words):\n",
        "    eng_idx = find_engineer_index(words)\n",
        "    if eng_idx != -1:\n",
        "        next_word = get_next_word(words, eng_idx)\n",
        "        return check_pronoun(next_word)\n",
        "    return \"neutral\"\n",
        "text = \"The engineer he fixed it\"\n",
        "words = split_text(text)\n",
        "result = analyze_bias(words)\n",
        "print(result)\n",
        "sample_text = \"The engineer she coded\"\n",
        "sample_words = split_text(sample_text)\n",
        "sample_result = analyze_bias(sample_words)\n",
        "print(sample_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Jppwt_FuDL-S",
        "outputId": "2235f107-d244-42c6-e58d-615c3f0dfa5b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "he\n",
            "she\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp 15 : Industry Application of NLP - Resume Parsing"
      ],
      "metadata": {
        "id": "0AM1Xx8YJbVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_skills_list():\n",
        "    return [\"python\", \"java\", \"nlp\", \"sql\"]\n",
        "\n",
        "def split_text(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "def find_skills(words, skills_list):\n",
        "    return [word for word in words if word in skills_list]\n",
        "\n",
        "def parse_resume(text, skills_list):\n",
        "    words = split_text(text)\n",
        "    return find_skills(words, skills_list)\n",
        "\n",
        "skills_list = get_skills_list()\n",
        "resume = \"I know Python and Java and NLP\"\n",
        "skills = parse_resume(resume, skills_list)\n",
        "print(skills)\n",
        "\n",
        "sample_resume = \"Experienced in SQL and Python coding\"\n",
        "sample_skills = parse_resume(sample_resume, skills_list)\n",
        "print(sample_skills)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Wn1VTkIa8BL8",
        "outputId": "fcbfdd99-6d74-4e37-dd3e-c104868b8561"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['python', 'java', 'nlp']\n",
            "['sql', 'python']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "c_HaNwYc_QIu",
        "outputId": "ba120d03-3e7a-40df-c1a5-d8a67e66d9de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "# Download the missing resource: averaged_perceptron_tagger_eng\n",
        "nltk.download('averaged_perceptron_tagger') # This usually downloads averaged_perceptron_tagger_eng\n",
        "# Add the explicit download for the specific resource\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "# Download the missing resource as suggested by the traceback\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "\n",
        "\n",
        "def tokenize_and_tag(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return nltk.pos_tag(tokens)\n",
        "\n",
        "def perform_ner(tagged_words):\n",
        "    return nltk.ne_chunk(tagged_words)\n",
        "\n",
        "text = \"Apple Inc. is an American multinational technology company headquartered in Cupertino, California.\"\n",
        "tagged_words = tokenize_and_tag(text)\n",
        "named_entities = perform_ner(tagged_words)\n",
        "print(named_entities)\n",
        "\n",
        "sample_text = \"Barack Obama was the 44th President of the United States.\"\n",
        "sample_tagged_words = tokenize_and_tag(sample_text)\n",
        "sample_named_entities = perform_ner(sample_tagged_words)\n",
        "print(sample_named_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "WDt0Zc2LHcw8",
        "outputId": "1ac8df7c-5224-4460-8dbd-96cf19736e33"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Apple/NNP)\n",
            "  (ORGANIZATION Inc./NNP)\n",
            "  is/VBZ\n",
            "  an/DT\n",
            "  (GPE American/JJ)\n",
            "  multinational/NN\n",
            "  technology/NN\n",
            "  company/NN\n",
            "  headquartered/VBD\n",
            "  in/IN\n",
            "  (GPE Cupertino/NNP)\n",
            "  ,/,\n",
            "  (GPE California/NNP)\n",
            "  ./.)\n",
            "(S\n",
            "  (PERSON Barack/NNP)\n",
            "  (PERSON Obama/NNP)\n",
            "  was/VBD\n",
            "  the/DT\n",
            "  44th/JJ\n",
            "  President/NNP\n",
            "  of/IN\n",
            "  the/DT\n",
            "  (GPE United/NNP States/NNPS)\n",
            "  ./.)\n"
          ]
        }
      ]
    }
  ]
}